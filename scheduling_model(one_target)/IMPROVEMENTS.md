# 模型收敛性改进方案

## 问题分析
原始设计中模型不收敛的主要原因：
1. **奖励惩罚过度**：线性递增惩罚导致负奖励累积过快
2. **训练超参数不当**：学习率过低(0.0005)、预热样本过多(5000)
3. **探索-利用平衡差**：目标网络更新频率过高(100步)

---

## 改进措施

### 1. 优化超参数 (train(2).py)
```python
# 原始设置 → 改进设置
MEMORY_WARMUP_SIZE: 5000 → 2000      # 加快初始训练
BATCH_SIZE: 64 → 128                 # 增大批大小稳定梯度
LEARNING_RATE: 0.0005 → 0.001        # 提高学习率加快收敛
GAMMA: 0.99 → 0.95                   # 降低gamma平衡时间视野
```

### 2. 改进奖励设计 (Envir.py)
#### 检测成功奖励
```python
# 基础奖励 + 距离奖励
reward = 10.0 + max(0, (range - dist) / range * 2)
# 结果：接近目标时 +12，远处 +10
```

#### 丢失惩罚（阶跃而非线性）
```python
if lost_steps == 0: reward = -2.0    # 第一次丢失：轻微
elif lost_steps == 1: reward = -5.0  # 第二次丢失：中等
else: reward = -8.0                  # 之后：较大
```
**优点**：避免过度惩罚，让模型有恢复机会

#### 动作切换惩罚（降低权重）
```python
# 保持动作: +2（鼓励稳定跟踪）
# 切换动作: -1（轻微惩罚，但不阻止必要切换）
```

### 3. 调整目标网络更新频率 (cartpole_agent.py)
```python
self.update_target_steps = 200  # 100 → 200
# 更新频率降低，使Q值估计更稳定
```

### 4. 基础惩罚系数调整 (Envir.py)
```python
loss_penalty_base = -5  # 不再使用，已改为阶跃方案
```

---

## 预期效果
- **更快的初始学习**：减少预热时间，更快进入有效训练
- **更稳定的收敛**：阶跃惩罚避免奖励崩溃，模型有恢复机会
- **更好的泛化**：平衡的奖励信号使策略更鲁棒
- **更合理的探索**：目标网络更新频率降低，Q值估计更可靠

---

## 调试建议

### 如果仍然不收敛：
1. **增加探索**：降低 `e_greed_decrement`（当前1e-4）到 5e-5
2. **增加网络容量**：在 cartpole_model.py 中增加隐层神经元
3. **尝试不同种子**：使用 `Env(seed=xxx)` 测试随机性影响
4. **监控奖励分布**：打印每个episode的平均奖励、最大/最小值

### 如果收敛但性能差：
1. **提升检测奖励**：增大距离奖励系数（当前×2）
2. **加强稳定性**：增大"保持动作"的奖励（当前+2）
3. **调整终止条件**：改变 `k_loss`（当前3）或 `max_steps`（当前200）

---

## 文件变更总结
- ✅ `Envir.py`: 改进奖励设计（阶跃惩罚、距离奖励）
- ✅ `train(2).py`: 优化超参数（学习率、批大小、预热）
- ✅ `cartpole_agent.py`: 调整目标网络更新频率
- ✅ `evaluate.py`: 自动同步 obs_dim=6

---

## 下一步
建议训练至少100个episode观察收敛趋势。如果仍未改善，可以：
1. 尝试不同的网络架构（如LSTM）
2. 实现优先经验回放（PER）
3. 考虑使用 A3C 或 PPO 等其他算法
